Date: Tue, 14 Jan 1997 21:25:51 GMT
Server: NCSA/1.4.2
Content-type: text/html
Last-modified: Mon, 04 Dec 1995 01:05:01 GMT
Content-length: 10777



Fortran D95 Compiler Overview








Fortran D95 Compiler Overview



Project Leaders:
John Mellor-Crummey
and Vikram Adve


Participants:
Zoran Budimlic,
Alan Carle,
Kevin Cureton,
Gil Hansen,
Ken Kennedy,
Charles Koelbel,
Bo Lu,
Collin McCurdy,
Nat McIntosh,
Dejan Mircevski,
Nenad Nedeljkovic,
Mike Paleczny,
Ajay Sethi,
Yoshiki Seo,
Lisa Thomas,
Lei Zhou



Parallel Compiler and Tools Group

Center for Research on Parallel Computation

Rice University




Group Mission:
Develop integrated compiler and tools to support effective machine-independent parallel programming





Contents
:


Compiler Goals


Fortran D95 Language


Fortran D95 Compiler Organization


Compiled Examples






Compiler Goals

Serve as an extensible platform for experimental research on compilation
techniques and programming tools for full applications, including:

unified treatment of regular and irregular problems
global strategies for computation partitioning
parallelism-enhancing and latency-hiding transformations
whole-program compilation and interprocedural transformations
code generation strategies that approach hand-tuned performance
architecture-independent compilation based on machine models
message-passing, shared-memory, and hybrid communication models
optimization in the presence of resource constraints
programming tools that fully support abstract, high-level, programming models






Fortran D95 Language

Fortran D95 is designed to support research on
data-parallel programming in High Performance Fortran (HPF) and to
explore extensions that would broaden HPF's applicability or enhance
performance.


Features

Fortran 77 + Fortran 90 array syntax + FORALL + ALLOCATE
High Performance Fortran (HPF) data mapping directives for regular problems
ALIGN, DISTRIBUTE, REALIGN, REDISTRIBUTE, TEMPLATE, PROCESSORS

INDEPENDENT, and ON_HOME
value-based data-mapping directives to support irregular problems
experimental support (under development) for
parallel input/output, including out-of-core array management
complex data structures
structured use of task parallelism










Fortran D95 Compiler Organization


Front End

Parallelism
Preliminary Communication Placement

and
Computation Partitioning
Communication

Placement
Communication Refinement


Code Generation







Front End


Purpose:
interpret HPF directives and compute directives affecting
each statement and reference

Directive Processing

semantic analysis of directives in program
infer canonical synthetic layout directives for
all program variables unmentioned in program directives
intraprocedural flow-sensitive propagation of
(RE)ALIGN, (RE)DISTRIBUTE to statements and array references


Limitations (November 1995)

no interprocedural propagation of layout information







Preliminary Communication Placement


Purpose:
provide feedback to the computation partitioner about where
(conservatively) communication might be needed

Strategy

conservatively assume all references to non-replicated variables
may need communication
hoist communication for a reference to the outermost loop level possible while
respecting data dependences on the reference or its subscripts
conservatively prevent communication from being hoisted out of
``non-do-loop'' iterative constructs


Limitations (November 1995)

placement independent of resource constraints
no support for pipelining communication to achieve partial parallelism
lacks dataflow placement optimization to
eliminate partial redundancies
hide communication latency

no inspector placement for irregular data accesses







Computation Partitioning Selection



Purpose:
a framework to evaluate and select from several computation partitioning
alternatives, not restricted to the owner-computes rule.

Approach:
explicitly enumerate candidate partitioning
choices and use explicit cost estimation to select the best partitioning.

enumerate candidate CP choices for a loop nest (or set of loop nests)
[example]

refine communication information for each candidate CP
estimate performance of each candidate CP:
load-balance (unimplemented)
communication overhead

propagate computation partitionings to DO, IF, statements, and
computations involving only privatizable variables.


Limitations (November 1995)


load balance is not considered
ignores message coalescing across loop nests
communication cost estimates are very simplistic
requires constant loop bounds (but simplistic handling of symbolics
will be straightforward)







Communication Refinement


Purpose:
given a computation partition choice, CP, compute a projection of
the conservatively placed communication w.r.t. CP:
[example]


eliminate communication for references local to that CP
eliminate redundant communication by coalescing
determine communication pattern
perform message coalescing optimization


Limitations (November 1995)


assume one single reaching layout per reference
conservative unless single processors statement,
perfect alignment, and same number of distributed dimensions
communication pattern recognition is somewhat limited
dataflow analysis for eliminating partial redundancies and
latency hiding not yet fully in place







Code Generation

Principal Functionality

[source for running example]


Computation partitioning transformations:

reduce loop bounds and insert guards where necessary
[example]

separate loop iterations that might access non-local values
to minimize overhead from runtime locality checks
[example]


Communication generation and storage management:

compute data sets to send/recv between processor pairs
generate code to pack/unpack buffers and send/recv data
[example]

generate run-time dynamic storage management to cope with dynamic layouts
[example]

localize and linearize subscripts
[example]





Current Strategy

Except for storage management, all the code generation tasks require
heavy manipulation of integer sets, especially for compiling regular
applications for distributed-memory machines. Examples:

data to send or recieve for a particular reference, given its
computation partition
a processor's loop iterations that access local or non-local data
Current implementation uses the Omega library (University of Maryland):
(+)

arbitrary integer sets
(+)

rich language for mappings between sets
(+)

almost complete set of operations on sets and mappings:
(union, intersection, difference, inverse, composition)
(+)

good code-generation and optimization
(-)

code generation is slow
(-)

limited support for symbolics


Limitations (November 1995)

run-time resolution guards currently handle only one or
all processors per dynamic statement instance
lack library support for dynamic remapping
current localization and linearization strategy produces
general, but slow code.







Compiled Examples

simple 1D shift kernel
[HPF]

[F77+MPI]

Jacobi iteration
[HPF]

[F77+MPI]

Livermore 18 explicit hydrodynamics kernel
[HPF]

[F77+MPI]



Non owner-computes partitioning fragment








http://www.cs.rice.edu/~mpal/SC95/index.html







