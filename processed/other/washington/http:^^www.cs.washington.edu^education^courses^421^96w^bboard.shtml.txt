Date: Mon, 02 Dec 1996 17:40:14 GMT
Server: NCSA/1.4.2
Content-type: text/html


CSE 421 Bboard/Mail Log



CSE 421 Formal Models
Bboard/Mail Log
Winter 1996
This page contains a log of all email sent to the CSE421 class
mailing list
cse421@cs
.  We will use this list for
announcements of general interest to the class.  Students should
also feel free to use it to ask questions, post information, or
initiate discussions  of general interest to the class.  Of course,
questions or comments that don't seem of general interest can be
directed to the TA (
aberman@cs
) or instructors
(
ruzzo@cs
or
tompa@cs
), instead.
Following usual Internet conventions, administrative requests
concerning the mailing list itself, such as add/delete/address
change requests, should be addressed to
cse421-request@cs
.
Index of Messages
(Latest message Friday, 15-Mar-96 14:49:40 PST.)


5 Jan 96 ruzzo@cs ______ cse421 mailing list

8 Jan 96 eallen@cs _____ study group

8 Jan 96 eallen@cs _____ Re: study group

8 Jan 96 ruzzo@cs ______ Homework #1 Bugs & Clarifications.

8 Jan 96 yihchun@u _____ Re: Homework #1 Bugs & Clarifications.

9 Jan 96 tompa@cs ______ HW1, problem 3

9 Jan 96 tompa@cs ______ Re: HW1, problem 3

12 Jan 96 ruzzo@cs ______ HW#2

12 Jan 96 aberman@cs ____ TA Office Hours

12 Jan 96 aberman@cs ____ Office Hours Conflict/Change

18 Jan 96 joeh@cs _______ 421 Book

20 Jan 96 ruzzo@cs ______ sorting animations

22 Jan 96 ruzzo@cs ______ Re: hw questions...

22 Jan 96 ruzzo@cs ______ Reading assignment

22 Jan 96 tompa@cs ______ further reading

23 Jan 96 ruzzo@cs ______ problem 8.4-4

23 Jan 96 tompa@cs ______ proofs of correctness

24 Jan 96 aberman@hobbes  Proof Tips

24 Jan 96 ruzzo@cs ______ expected time for insertion sort

25 Jan 96 ruzzo@cs ______ Re: (CSE421) Problems using induction to

25 Jan 96 tompa@cs ______ Re: (CSE421) Problems using induction to

29 Jan 96 tompa@cs ______ CSE 421 lecture cancelled today

31 Jan 96 tompa@cs ______ more on the (in)efficiency of the select

1 Feb 96 eallen@cs _____ problem 5

1 Feb 96 michael@u _____ Tutors for 421?

1 Feb 96 wbarrett@cs ___ So HOW BAD IS IT, Really?

2 Feb 96 tompa@cs ______ HW4, problem 2

2 Feb 96 tompa@cs ______ elephant humor

2 Feb 96 tompa@cs ______ HW4, problem 5

4 Feb 96 claym@wolf ____ Does problem 3 suck or am I just dense?

4 Feb 96 tompa@cs ______ HW4, problem 3

7 Feb 96 ruzzo@cs ______ wednesday office hour CANCELED

8 Feb 96 tompa@cs ______ HW5, problem 2(a)

8 Feb 96 aberman@cs ____ Average Homework Grades

9 Feb 96 ruzzo@cs ______ office hours

9 Feb 96 tompa@cs ______ Cocke, Kasami, Younger algorithm

9 Feb 96 ogan@cs _______  mailing list

11 Feb 96 dgodon@u ______ Re: Cocke, Kasami, Younger algorithm

12 Feb 96 tompa@cs ______ HW5, problem 3a

12 Feb 96 tompa@cs ______ next reading assignment

12 Feb 96 aberman@hobbes  Average for HW#4

14 Feb 96 aberman@cs ____ Mistake in Homework Grading

19 Feb 96 tompa@cs ______ HW6 on the web

22 Feb 96 aberman@hobbes  Office Hours *CANCELLED* This Monday and

22 Feb 96 tompa@cs ______ HW6, #2 (Warning: this message contains

25 Feb 96 eallen@cs _____ last problem

25 Feb 96 ruzzo@cs ______ Re: last problem

26 Feb 96 tompa@cs ______ last problem

26 Feb 96 claym@cs ______ Re: HW6, #2 (Warning: this message conta

26 Feb 96 tompa@cs ______ "describe an algorithm"

26 Feb 96 tompa@cs ______ space

26 Feb 96 tompa@cs ______ HW6, #4

3 Mar 96 tompa@cs ______ details on Chapter 36 reading assignment

3 Mar 96 ruzzo@cs ______ 24-1(a)

3 Mar 96 ruzzo@cs ______ Re: 24-1(a)

3 Mar 96 ruzzo@cs ______ HW7, 17.2-4

4 Mar 96 ruzzo@cs ______ HOMEWORK REVISION

4 Mar 96 ruzzo@cs ______ OFFICE HOUR CHANGE

5 Mar 96 ruzzo@cs ______ Re: HW 7, 17-2.2  (***CAUTION; HINT BELO

6 Mar 96 tompa@cs ______ evaluations Friday

7 Mar 96 tompa@cs ______ HW8 (just kidding)

8 Mar 96 aberman@hobbes  Office Hours During Finals Week

13 Mar 96 roske@cs ______ Re: HW8 (just kidding)

13 Mar 96 jshill@u ______ Re: HW8 (just kidding)

13 Mar 96 tompa@cs ______ HW8 sketchy solutions

13 Mar 96 tompa@cs ______ HW7

13 Mar 96 tompa@cs ______ Problem 36.1-4

13 Mar 96 eallen@cs _____ NP

14 Mar 96 aberman@hobbes  Hw7

15 Mar 96 tompa@cs ______ HW7 and final exam pickup




Messages


Date: 5 Jan 1996 17:21 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
cse421 mailing list

We have established a mailing list, cse421@cs, for class information
and discussion.  Initial email addresses are taken from the
registrar's 1st day class list, hence are all "...@u" addresses.
Please set up mail forwarding from that account if you don't
normally read mail there.  Mail sent to the list is also
automatically logged to the course web,
http://www.cs.washington.edu/education/courses/421

Send mail to cse421-request@cs to be added to or removed from the
list.
Date: Mon, 8 Jan 1996 09:53:10 -0800 (PST)
From:
Eva Marie Allen <eallen@wolf.cs.washington.edu>
To: cse421@cs
Subject:
study group

Hi everyone,

Several people have started a study group, and want to invite others to
join.  The meetings times thus far are Monday and Friday, 8:30 to 10:30.
Early?  Yes.  But then, painful times call for painful hours.  :)

Eva
Date: Mon, 8 Jan 1996 13:39:01 -0800 (PST)
From:
Eva Marie Allen <eallen@wolf.cs.washington.edu>
To: Cele Couch <cele@grizzly.cs.washington.edu>
cc: cse421@cs
Subject:
Re: study group

On Mon, 8 Jan 1996, Cele Couch wrote:

> where do you guys meet?  the lounge?
>
>
Yes, so far in the Undergrad lounge.

Eva
Date: 8 Jan 1996 17:01 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
Homework #1 Bugs & Clarifications.

1. pg 15 1.3-3:  Assume n = 2^ k for some integer k>=0.

3. pg 37 2.2-2:  Assume the domain of T(n) is the set of integers n
>= 2.  (A careful reading of the definitions seems to show the
"if" direction is false otherwise.)

4. pg 37 2.2-3:  "Equation (2.9)" just means the last of the 7
equations.

5. pg 37 2.2-8: Use the recurrence (2.13) and induction.  You may
NOT use equation (2.15) (or problem 2.2-7).
Date: Mon, 8 Jan 1996 22:12:17 -0800 (PST)
From:
Yih-Chun Hu <yihchun@u.washington.edu>
To: Larry Ruzzo <ruzzo@cs.washington.edu>
Cc: cse421@cs.washington.edu
Subject:
Re: Homework #1 Bugs & Clarifications.

On 8 Jan 1996, Larry Ruzzo wrote:

> 3. pg 37 2.2-2:  Assume the domain of T(n) is the set of integers n
>     >= 2.  (A careful reading of the definitions seems to show the
>     "if" direction is false otherwise.)

The definition given on page 26 of the Big Book of Knowledge seems to
indicate that the function 0 (the constant function with value zero) is in
O(n), thus there exists a k such that 0 is in O(n^k). It therefore
follows that the function T(n) = 0 should be in n^{O(1)} (if the
corrected version of the problem is correct). However, one should note
that this is not the case when n >= 2. (Proof: Suppose there exists
function g(x) st n^g(x) = 0 for all n >= 2. Then g(n) gives us a
base n logarithm of zero.) It therefore follows that either 0 is not in
O(n^k) [contradiction of definition on page 26] or it is possible to take
logarithms of zero.


+---- Yih-Chun Hu (finger:yihchun@cs.washington.edu) ----------------------+
| http://www.cs.washington.edu/homes/yihchun     yihchun@cs.washington.edu |
| http://weber.u.washington.edu/~yihchun         yihchun@u.washington.edu  |
+--------------------------------------------------------------------------+
To: Yih-Chun Hu <yihchun@u.washington.edu>
cc: cse421@cs.washington.edu
Subject:
HW1, problem 3
Date: Tue, 09 Jan 1996 15:55:33 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Thanks for pointing out this further problem with n^{O(1)}.  I believe you are
right, that if T(n) = 0 for any value of n (say, n=3), then there is no
function g(n) in O(1) such that 0 = T(3) = 3 ^ {g(3)}.

For this problem, then, it's o.k. to assume that T(n) > 0 for all n.

Sorry that the problem has yet another bug.

------- Forwarded Message

Date:    Mon, 08 Jan 1996 22:12:17 -0800
From:    Yih-Chun Hu <yihchun@u.washington.edu>
To:      Larry Ruzzo <ruzzo@cs.washington.edu>
cc:      cse421@cs.washington.edu
Subject: Re: Homework #1 Bugs & Clarifications.


On 8 Jan 1996, Larry Ruzzo wrote:

> 3. pg 37 2.2-2:  Assume the domain of T(n) is the set of integers n
>     >= 2.  (A careful reading of the definitions seems to show the
>     "if" direction is false otherwise.)

The definition given on page 26 of the Big Book of Knowledge seems to
indicate that the function 0 (the constant function with value zero) is in
O(n), thus there exists a k such that 0 is in O(n^k). It therefore
follows that the function T(n) = 0 should be in n^{O(1)} (if the
corrected version of the problem is correct). However, one should note
that this is not the case when n >= 2. (Proof: Suppose there exists
function g(x) st n^g(x) = 0 for all n >= 2. Then g(n) gives us a
base n logarithm of zero.) It therefore follows that either 0 is not in
O(n^k) [contradiction of definition on page 26] or it is possible to take
logarithms of zero.


+---- Yih-Chun Hu (finger:yihchun@cs.washington.edu) ----------------------+
| http://www.cs.washington.edu/homes/yihchun     yihchun@cs.washington.edu |
| http://weber.u.washington.edu/~yihchun         yihchun@u.washington.edu  |
+--------------------------------------------------------------------------+


------- End of Forwarded Message
To: Yih-Chun Hu <yihchun@u.washington.edu>
cc: cse421@cs.washington.edu
Subject:
Re: HW1, problem 3
Date: Tue, 09 Jan 1996 16:16:01 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

You're right again: I was thinking of T(n) as mapping integers to integers,
which isn't necessarily the case.  So make the assumption T(n) >= 1 for all n.

------- Forwarded Message

Date:    Tue, 09 Jan 1996 16:00:25 -0800
From:    Yih-Chun Hu <yihchun@u.washington.edu>
To:      Martin Tompa <tompa@cs.washington.edu>
cc:      ruzzo@cs.washington.edu
Subject: Re: HW1, problem 3


Is this fix sufficient? If I have T(n) = 0.5, we will have a negative g(n).
But then the negative g(n) is not in O(h(n)) for any function h(n) by
definition of O(h(n)).

On Tue, 9 Jan 1996, Martin Tompa wrote:

>
> Thanks for pointing out this further problem with n^{O(1)}.  I believe you ar
e
> right, that if T(n) = 0 for any value of n (say, n=3), then there is no
> function g(n) in O(1) such that 0 = T(3) = 3 ^ {g(3)}.
>
> For this problem, then, it's o.k. to assume that T(n) > 0 for all n.
>
> Sorry that the problem has yet another bug.
>
> ------- Forwarded Message
>
> Date:    Mon, 08 Jan 1996 22:12:17 -0800
> From:    Yih-Chun Hu <yihchun@u.washington.edu>
> To:      Larry Ruzzo <ruzzo@cs.washington.edu>
> cc:      cse421@cs.washington.edu
> Subject: Re: Homework #1 Bugs & Clarifications.
>
>
> On 8 Jan 1996, Larry Ruzzo wrote:
>
> > 3. pg 37 2.2-2:  Assume the domain of T(n) is the set of integers n
> >     >= 2.  (A careful reading of the definitions seems to show the
> >     "if" direction is false otherwise.)
>
> The definition given on page 26 of the Big Book of Knowledge seems to
> indicate that the function 0 (the constant function with value zero) is in
> O(n), thus there exists a k such that 0 is in O(n^k). It therefore
> follows that the function T(n) = 0 should be in n^{O(1)} (if the
> corrected version of the problem is correct). However, one should note
> that this is not the case when n >= 2. (Proof: Suppose there exists
> function g(x) st n^g(x) = 0 for all n >= 2. Then g(n) gives us a
> base n logarithm of zero.) It therefore follows that either 0 is not in
> O(n^k) [contradiction of definition on page 26] or it is possible to take
> logarithms of zero.
>
>
> +---- Yih-Chun Hu (finger:yihchun@cs.washington.edu) ----------------------+
> | http://www.cs.washington.edu/homes/yihchun     yihchun@cs.washington.edu |
> | http://weber.u.washington.edu/~yihchun         yihchun@u.washington.edu  |
> +--------------------------------------------------------------------------+
>
>
> ------- End of Forwarded Message
>
>

+---- Yih-Chun Hu (finger:yihchun@cs.washington.edu) ----------------------+
| http://www.cs.washington.edu/homes/yihchun     yihchun@cs.washington.edu |
| http://weber.u.washington.edu/~yihchun         yihchun@u.washington.edu  |
+--------------------------------------------------------------------------+


------- End of Forwarded Message
Date: 12 Jan 1996 04:21 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
HW#2

HW#2 will be passed out in class today, and is also on the web, for
those of you who want to get a start on it this morning.
To: cse421@cs
Subject:
TA Office Hours
Date: Fri, 12 Jan 1996 11:07:34 PST
From:
Andrew Berman <aberman@paintbrush.cs.washington.edu>

I have scheduled Office Hours for Monday and Tuesday, 9:30-10:30 am
in room 326a.

Andy




_______________________________________________________________________________
|    Andrew P. Berman     |Dept. Of Computer Science, University Of Washington|
|aberman@cs.washington.edu|  http://www.cs.washington.edu/homes/aberman       |
|_________________________|___________________________________________________|
To: cse421@cs
Subject:
Office Hours Conflict/Change
Date: Fri, 12 Jan 1996 12:04:18 PST
From:
Andrew Berman <aberman@paintbrush.cs.washington.edu>

I've just been alerted that my Monday morning office hours conflicts with
a class which many students may be taking. Henceforth, my Monday office hours
are from 11:30 to 12:20.

New Schedule:  Monday 11:30-12:30, Tuesday 9:30-10:30


Andy



_______________________________________________________________________________
|    Andrew P. Berman     |Dept. Of Computer Science, University Of Washington|
|aberman@cs.washington.edu|  http://www.cs.washington.edu/homes/aberman       |
|_________________________|___________________________________________________|
Date: Thu, 18 Jan 1996 14:11:51 -0800 (PST)
From:
Joseph Heitzeberg <joeh@wolf.cs.washington.edu>
To: cse421@wolf.cs.washington.edu
Subject:
421 Book

421 -

I forgot my 421 book in Sieg 326 today, Did anyone happen to see it?
If so please let me know, I'de really appreciate it (because it's so
ex$pen$ive)

Thanks,
Joe Heitzeberg
Date: 20 Jan 1996 18:30 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
sorting animations

If any of you happen to have access to a web browser supporting Java
applets, you might enjoy seeing their sorting demo applet:
http://java.sun.com/applets/applets/SortDemo/example1.html
Date: 22 Jan 1996 07:39 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
Re: hw questions...

> When the book asks for us to 'argue' something does it want a formal
> proof or does it want a intuitive explanation?  I'm specifically
> referring to the first two questions.

I take "argue" and "prove" to be synonymous.

on problem 8.4-4, the requested analysis is the analysis of the
expected (average) time for randomized quicksort (i.e., quicksort
with random selection of pivot), modified as suggested in the
problem.  We'll give partial credit for correct analysis of
worst-case time of this variant of *deterministic* quicksort with
exact (linear time) median.  Using a recursion tree to analyze the
recurrence might be easiest in this case.
Date: 22 Jan 1996 15:11 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
Reading assignment

here's an updated reading assignment, for material up to and
including today's lecture.  (You'll get a hardcopy of this with your
next homework assignment.)

\begin{itemize}
\item Ch. 3 (Summations): We've seen examples of most of
the techniques in this chapter by now.  This would
be a good time to read it if you haven't already.
\item Ch. 4 (Recurrences): ditto, except you may skip 4.4.
\item Ch. 8 (Quicksort): read all of it.
\end{itemize}
To: cse421@geoduck
Subject:
further reading
Date: Mon, 22 Jan 1996 15:31:13 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Wednesday we'll dive into Chapter 10.  Read the whole chapter, concentrating
on 10.3.

The remaining divide-and-conquer algorithm that was on the syllabus is the
fast Fourier transform, from Chapter 32.  We will skip this for the moment,
and possibly return to it later in the quarter.

So the next topic after Chapter 10 is Chapter 16 (following the syllabus).
Date: 23 Jan 1996 14:30 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
problem 8.4-4

a student pointed out that many of you looked at a version of
quicksort in 326 that switched to doing insertion sort when the
problem size was less than some small k, so in total you'll do at
least n/k insertion sorts on small subproblems of size at most k.
Although related, this is NOT the version that you are to analyze in
8.4-4.  Rather than switching to insertion sort at the bottom of the
recursion, 8.4-4 says DO NOTHING when the subproblems are of size
<=k; just return, leaving that subproblem unsorted.  Patch up ALL of
these small unsorted subproblems with ONE global insertion sort at
the end.  [This has a slight advantage in practice, since you only
setup & run the insertion-sort loops once, rather than n/k times.]
To: cse421@geoduck
Subject:
proofs of correctness
Date: Tue, 23 Jan 1996 15:00:43 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

A student asked for suggestions or models of how to do proofs of correctness
of algorithms.  In particular "Stooge sort" and Quicksort' on the homework ask
for correctness proofs.

In both of these cases, the fact that a problem is being reduced to smaller
problems (either by recursion or by iteration) suggests a proof by induction
on the size of the problem.  Using induction, I think you can come up with a
reasonably rigorous proof in each case.

In the case of Quicksort', there are at least two ways of applying induction:
you can either appeal to the induction hypothesis once per iteration (for the
recursive call), or you can have exactly two appeals to the induction
hypothesis (one for the first recursive call, and one for what's accomplished
together by all iterations after the first).
Date: Wed, 24 Jan 1996 11:57:33 -0800
From:
aberman@hobbes (Andrew Berman)
To: cse421@hobbes
Proof Tips: CSE 421

Here are some tips to writing good readable proofs.  This document is not a
complete guide to proofwriting (we'd like to one of those for ourselves). We
hope, however, that this document will help people who are a little unsure of
what a good proof looks like.


1) Proofs are communications:
A proof is not an abstract mathematical construct.  It is a very
concrete and specific communication.  Specifically, you
are communicating to others that they should believe something.  Most
of the points in this note follow from this paradigm.


2)  Include Commentary
Sometimes students get the idea that if it isn't an equation,  it shouldn't be
in the proof.  That's not true.  While a comment doesn't substitute for a
logical inference, it can help the reader understand what logical inference
you are using.


3) Organization
A good proof will be easy to read and understand.  Organization helps.  Think
of the proof as a path for the reader to follow. Tell the reader what the goals
or subgoals are, and then show the reader how these goals are reached.
Here's a simple example:

To prove that X = Z, we will first show that X = Y, and then we will
show Y=Z.
1) We show that X = Y by induction
....
2) We now show that Y = Z by contradiction....

Since X = Y from (1) and Y = Z from (2), by transitivity, X = Z.


4) Label things:
If you are going to use a standard method such as induction, you must use the
conventions of the method.  In an inductive proof, that includes labeling the
base case, inductive hypothesis and inductive steps as such.


5)  Don't use the "reduction to 1=1" strategy.
Strategies such as induction are useful because there is a guarantee that if
you use the strategy correctly then you will wind up with a correct proof. One
strategy sometimes used to prove X=Y is to start off with X=Y and reduce the
equation to something trivially correct, such as 1=1.  This strategy is invalid
because it does not offer the guarantee that using it correctly results in a
proof. For example:

1 = -1
1^2 = (-1)^2
1 = 1   QED

The squaring step is not reversible which is why this doesn't work.  Yet the
strategy does not flag this problem.  If you are thinking in these terms, try
starting from 1=1 and deducing X=Y.  Then you are using the valid strategy of
starting with something true and using legitimate mathematical operations which
maintain truthfulness.


7) Look at boundary cases:
Is what you are saying true for N=0? How about N=1? what about as N->inf? What
about negative or non-integer N? Several problems of this sort cropped up in
question 2.2-2, and will probably crop up in later problems as well.  If you
see something like this, you can ask us about it, or, you can state that you
are limiting your domain to positive N, or integer N, or something like that.
It's far better to limit the scope of your proof than to write something false.

8) Don't skip steps:
It can be hard to know how much detail you should put down,
but here's some examples of how to reduce one equation to another:

Too Little:
[2^(k+1)]/2 = N
k log (2) = log(N)
k	  = log(N)

Minimimally acceptable:
[2^(k+1)]/2 = N
2^(k)	  = N
k	  = log(N)
Better:
[2^(k+1)]/2]= N
2^(k)	  = N
log(2^(k))=log(N) [base 2]
k	  = log(N)

Note that the first two examples have the same number of lines.  However,
the first one had a leap between the first two lines which would force
the reader to puzzle over it for a while.  In general, limit each new line
to one mathematical operation.


9) Try to keep it compact:
Part of clarity is compactness.  If your proof starts to meander and look more
like an essay, try rewriting it in fewer sentences. If you can't rewrite it and
it still meanders, perhaps you're missing some key element.  Compactness is not
in conflict with point (8) as long as you remember that clarity is the overall
goal.  Either too much or too little verbiage can be detrimental.


10) A Note About Inductive Proofs:
I noticed that some people don't use the correct base case.
Consider the statement of the first problem:
T(N) = 2 if N=2
= 2T(N/2) + N if N > 2
Prove that T(N) = N log N.

The base case is N=2, not N=4.  I think perhaps since N=2 is listed separately
in the description of T(N), people may be worried that the induction "might not
work" that far.  But it does--that's the whole point of having the base case
and of listing it separately in the original description. In general, try to
make the proofs as encompassing as possible.
Date: 24 Jan 1996 15:06 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
expected time for insertion sort

Andy poins out that as part of the answer to  8.4-4 on page 167, you
seemingly have to prove something about the expected case running
time for insertion sort.  That's correct, but it turns out that a
simple upper bound on the worst-case running time is all you need,
so this isn't too hard.  In other words, unlike the quicksort part,
you do NOT need to  try to set up or solve a recurrence for the
expected time of the insertion part.
Date: 25 Jan 1996 00:41 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
Re: (CSE421) Problems using induction to prove correctness

> I'm having difficulty making the leap from base case and inductive hyp.
> to the inductive step for the case when n+1.  In other words, I can
> convince myself that StoogeSort works for length 1 or three, and assume
> that it works for length n, but I just don't see anyway to reason about
> sorting length n+1.
>
> Are there any hints you could offer or am I approaching this wrong?

Approach sounds right.

Hint one: forget (temporarily) that it's recursion/induction;
generally all you should need to assume about what goes on in the
recursive calls is THAT they sort; you shouldn't need to ever know
HOW they sort.  (That's the beauty of induction...)

Hint two: work from some examples.  What happens if the input is
already sorted?  what happens if the input is reverse-sorted?  What
happens if it's sorted but some particular 2 elements are out of
order?  Can you argue in general that the smallest element ends up in
position 1? the 5th smallest in position 5?  The largest in position
n?  Turning in scattered examples like these won't get full credit;
that's not the point.  But hopefully they will help you understand
what the algorithm is doing so that you can generalize from these
specific examples to a comprehensive correctness argument.
To: cse421@geoduck
Subject:
Re: (CSE421) Problems using induction to prove correctness
Date: Thu, 25 Jan 1996 18:11:05 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Another possible stumbling block that may be causing this kind of problem is
using the wrong form of induction.  (Just a guess from the way the question
below was worded.) You don't want to proceed from n to n+1, but rather from
about 2n/3 to n.  This suggests using the strong form of induction rather than
ordinary induction.

------- Forwarded Message

> Date:    25 Jan 1996 00:41:00 -0800
> From:    Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
> To:      cse421@cs
> Subject: Re: (CSE421) Problems using induction to prove correctness
>
>
>   > I'm having difficulty making the leap from base case and inductive hyp.

>   > to the inductive step for the case when n+1.  In other words, I can
>   > convince myself that StoogeSort works for length 1 or three, and assume

>   > that it works for length n, but I just don't see anyway to reason about

>   > sorting length n+1.
>   >
>   > Are there any hints you could offer or am I approaching this wrong?
>
> Approach sounds right.
>
> Hint one: forget (temporarily) that it's recursion/induction;
> generally all you should need to assume about what goes on in the
> recursive calls is THAT they sort; you shouldn't need to ever know
> HOW they sort.  (That's the beauty of induction...)
>
> Hint two: work from some examples.  What happens if the input is
> already sorted?  what happens if the input is reverse-sorted?  What
> happens if it's sorted but some particular 2 elements are out of
> order?  Can you argue in general that the smallest element ends up in
> position 1? the 5th smallest in position 5?  The largest in position
> n?  Turning in scattered examples like these won't get full credit;
> that's not the point.  But hopefully they will help you understand
> what the algorithm is doing so that you can generalize from these
> specific examples to a comprehensive correctness argument.

------- End of Forwarded Message
To: cse421@geoduck
cc: receptionist@cs.washington.edu
Subject:
CSE 421 lecture cancelled today
Date: Mon, 29 Jan 1996 10:01:30 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Because of the icy roads and what I hear is low student attendance on
campus today, I'm cancelling lecture and my office hour today.  I
apologize to those of you who struggled to get to campus.  See you on
Wednesday, I hope.
To: cse421@geoduck
Subject:
more on the (in)efficiency of the selection algorithm
Date: Wed, 31 Jan 1996 15:41:49 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

I was somewhat dismayed in lecture today about the fact that the linear time
selection algorithm only seemed to use fewer comparisons than sorting when n >
2 ^ (80d) = 2 ^ 240.  This is obviously totally impractical.  (I hope you
realized that when I said we wouldn't be doing any useful algorithms in this
class, I was being facetious.  Although the stress is on elegant algorithms
with good theoretical analyses, we wouldn't be doing much good if they or
their techniques weren't also useful in practice.  For most of the algorithms
we cover, I hope you will find them practical as well as elegant.)

I gave the analysis more thought, and with some care you can get the constant
down from 240 to nearly 24.  This comes from two sources:

(1) A careful analysis shows that d = 2.4 -- the overhead is 1.4n for sorting
the blocks of size 5, and n+1 for partitioning about x.

(2) As Yih-Chun suggested, raising the basis of the recurrence beyond 80 pays
off.  Remember that, in determining the constant c, I had an expression that
was 80d when n=80 and decreased monotonically to 10d in the limit?  By raising
the basis, you can get as close to 10d as you like, and it doesn't happen too
slowly.  Of course, raising the basis also affects d, so there's a tradeoff.

Putting these together, we end up with an algorithm whose number of
comparisons is slightly more than 10dn = 24n.

The good news is that 24n is a respectable running time, much better than the
conservative 240n derived in class.  The bad news is that this still only
beats mergesort when n > 2^24 > 10^7.  But this says more about how very
efficient an nlogn time algorithm is, rather than how inefficient a 24n time
algorithm is.  For instance, this 24n time selection algorithm would beat
insertion sort (in number of comparisons) when n > 48.

Using blocks of size 15 and more clever tricks, the original 1972 paper by
Blum, Floyd, Pratt, Rivest, and Tarjan provided an upper bound of 5.43n
comparisons.  The best upper bound to date is 3n comparisons, by Scho"nhage,
Paterson, and Pippenger (1976), using quite different techniques.  The best
lower bound to date for the median problem is 2n comparisons, by Bent and John
(1985).
Date: Thu, 1 Feb 1996 14:41:14 -0800 (PST)
From:
Eva Marie Allen <eallen@wolf.cs.washington.edu>
To: Algorithms <cse421@cs.washington.edu>
Subject:
problem 5

Hi,

The longer I spend on this homework, the more I feel like I need to go
back to kindergarten.

The definition of median is the ceiling(n+1/2) largest element.
Correct?  Doesn't this mean that any of the elements of an
array are the median, since they all have the same value?  The weighted
median, on the other hand is very specific.  Is this how I should see
this problem?

Thanks,

Eva
Date: Thu, 1 Feb 1996 17:28:44 -0800 (PST)
From:
Michael Baker <michael@u.washington.edu>
To: cse421@cs.washington.edu
Subject:
Tutors for 421?

Because of work and a bout with sickness, I've fallen drastically behind
in this course and am at the point where I should either drop it or find a
tutor.  Not being much of a quitter and this being my last quarter at the
U, I'd prefer the latter case.

Anyone out there willing to do some tutoring for 421?  Willing to pay
well.  Drop me a line via e-mail.  Thanks in advance.

Michael

Michael Baker  ---  michael@u.washington.edu  ---  michaelb@cs.washington.edu

Come in from the cold, I'll owe you my heart,
Be my shelter and refuge for the night.           Fields of the Nephilim
Love of my life, pour your light,               (Paradise Regained)
On the faith I can feel, make it real....
Date: Thu, 1 Feb 1996 17:50:47 -0800 (PST)
From:
Wade Barrett <wbarrett@wolf.cs.washington.edu>
To: cse421@wolf.cs.washington.edu
Subject:
So HOW BAD IS IT, Really?

I did a bunch of number-crunching on my $5.00 (if that) calculator to try
to figure out WHEN (for what value of n) the dazzlingly linear-time
SELECT algorithm REALLY does its job with fewer comparisons than simply
n*lg(n) sorting the n elements and indexing the one you want.

Here's the formula I crunched:     For what value of n is:

|n/5| lg |n/5|  +  (7n/10 + 6) lg (7n/10 + 6)  +  2.4 n   <   n*lg(n)   ?

Here's why:

One of our recent homework problems suggested a sorting algorithm that
QuickSorted n elements (recursively) until the size of the subproblem was
less than k for some k.  The inequality I wrote above implicitly ASSUMES
that for the value of n being tested, I will compare the n*lg(n) sort (on
the right) with a NON-RECURSIVE version of SELECT.  In other words, see
if it pays off to go through the SELECT routine JUST ONCE.  I submit that
the smallest value of n that makes the above inequality true is EXACTLY
the crossover point (counting the number of comparisons) at which one
should benefit from using the SELECT routine instead of sorting all n
elements.

Details of the formula:

1.4n                        ...  Sort the |n/5| baby groups of 5 each
|n/5| lg |n/5|              ...  Sort the medians of the |n/5| groups
n                           ...  Partition the n elements about the x
(7n/10 + 6) lg (7n/10 + 6)  ...  Sort the group containing the median

The 1.4n comes from the fact that 5 elements can be sorted with 7
comparisons, giving 7 * |n/5| = 1.4n comparisons.  (In fact, the ceiling
can be dropped since the leftover (m = 1, 2, 3, or 4) elements always
require less than 1.4*m comparisons to sort.)


HERE'S THE ANSWER:

By somewhere around n = 60,000, it requires less comparisons to use the
SELECT algorithm than to sort the whole array.  (Better value, anybody?)

So I conclude that the selection problem should be solved by:
-->  sorting and indexing if n < 60000,
-->  calling SELECT      if n >= 60000.
Note that this method should be the guide for recursive calls as well.


If the above is correct, then it isn't at all as bad as we
thought.  We actually COULD USE this algorithm beneficially for
REASONABLY large n.  (However, note that this analysis only compares
the numbers of *comparisons* used by the two methods.  Relative
differences in the amount of other work the algorithms need to do could
shift the tradeoff point quite a bit in either direction.  I think it
would be great if somebody in the class would actually test this out.
Are we IN FACT doing anything practically worthwhile here (other than
mutilating our brains) ??


On Wed, 31 Jan 1996, Martin Tompa wrote:

>
> I was somewhat dismayed in lecture today about the fact that the linear time
> selection algorithm only seemed to use fewer comparisons than sorting when n >
> 2 ^ (80d) = 2 ^ 240.  This is obviously totally impractical.  (I hope you
> realized that when I said we wouldn't be doing any useful algorithms in this
> class, I was being facetious.  Although the stress is on elegant algorithms
> with good theoretical analyses, we wouldn't be doing much good if they or
> their techniques weren't also useful in practice.  For most of the algorithms
> we cover, I hope you will find them practical as well as elegant.)
>
> I gave the analysis more thought, and with some care you can get the constant
> down from 240 to nearly 24.  This comes from two sources:
>
> (1) A careful analysis shows that d = 2.4 -- the overhead is 1.4n for sorting
> the blocks of size 5, and n+1 for partitioning about x.
>
> (2) As Yih-Chun suggested, raising the basis of the recurrence beyond 80 pays
> off.  Remember that, in determining the constant c, I had an expression that
> was 80d when n=80 and decreased monotonically to 10d in the limit?  By raising
> the basis, you can get as close to 10d as you like, and it doesn't happen too
> slowly.  Of course, raising the basis also affects d, so there's a tradeoff.
>
> Putting these together, we end up with an algorithm whose number of
> comparisons is slightly more than 10dn = 24n.
>
> The good news is that 24n is a respectable running time, much better than the
> conservative 240n derived in class.  The bad news is that this still only
> beats mergesort when n > 2^24 > 10^7.  But this says more about how very
> efficient an nlogn time algorithm is, rather than how inefficient a 24n time
> algorithm is.  For instance, this 24n time selection algorithm would beat
> insertion sort (in number of comparisons) when n > 48.
>
> Using blocks of size 15 and more clever tricks, the original 1972 paper by
> Blum, Floyd, Pratt, Rivest, and Tarjan provided an upper bound of 5.43n
> comparisons.  The best upper bound to date is 3n comparisons, by Scho"nhage,
> Paterson, and Pippenger (1976), using quite different techniques.  The best
> lower bound to date for the median problem is 2n comparisons, by Bent and John
> (1985).
>
To: cse421@geoduck
Subject:
HW4, problem 2
Date: Fri, 02 Feb 1996 08:52:55 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Problem 2 is quite a bit easier than most other problems we ask, so don't look
desperately for a "trick" if you solved it easily.  However, if you're solving
it recursively, you're not doing it nearly as efficiently (or as simply) as
you could.
To: cs421@geoduck
Subject:
elephant humor
Date: Fri, 02 Feb 1996 09:01:35 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

This was a small part of one of those very long jokes that you can't wait to
get to the end of.

>>COMPUTER SCIENTISTS hunt elephants by exercising Algorithm A:
>>      1. Go to Africa.
>>      2. Start at the Cape of Good Hope.
>>      3. Work northward in an orderly manner, traversing the continent
>>         alternately east and west.
>>      4. During each traverse pass,
>>            a. Catch each animal seen.
>>            b. Compare each animal caught to a known elephant.
>>            c. Stop when a match is detected.
>>
>>EXPERIENCED COMPUTER PROGRAMMERS modify Algorithm A by placing a known
>>elephant in Cairo to ensure that the algorithm will terminate.
To: cse421@geoduck
Subject:
HW4, problem 5
Date: Fri, 02 Feb 1996 16:39:35 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

The book talks about THE weighted median, but a few students pointed out that
it may not be unique, for instance, if w1 = 1/6, w2 = 2/6, and w3 = 3/6.  It's
not too hard to prove that there will either be 1 or 2 weighted medians.  If
there happen to be two, let's agree that THE weighted median is the larger of
those two, to agree with the way we've disambiguated the ordinary median in
the case when there are an even number of elements.
Date: Sun, 4 Feb 1996 13:13:20 -0800
From:
claym@wolf (Michael Clay)
To: cse421@cs.washington.edu
Subject:
Does problem 3 suck or am I just dense?
Cc: claym@wolf
Seems like most of the homework is straightforward, albeit a bit
beyond lengthy, except problem 3.

All my attempts at a solution to #3 so far are convoluted and seem to run
in O(lg**2 n) time.  I must be missing something, either obvious or obscure.
Any hints that would steer me in the right direction without totally
giving away the solution would be greatly appreciated.

Thanks,
Michael (claym@wolf.cs.washington.edu)
(But post your answers to cs421 distribution.)
To: cse421@geoduck
Subject:
HW4, problem 3
Date: Sun, 04 Feb 1996 14:16:37 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Given that you're after an O(log n) solution, something to shoot for
is to throw away about half the problem every constant number of
steps.

Personally, I didn't find problem 5 straightforward, and I know some
students are having trouble with problem 1.

------- Forwarded Message

Date:    Sun, 04 Feb 1996 13:13:20 -0800
From:    claym@wolf (Michael Clay)
To:      cse421@cs.washington.edu
cc:      claym@wolf
Subject: Does problem 3 suck or am I just dense?

Seems like most of the homework is straightforward, albeit a bit
beyond lengthy, except problem 3.

All my attempts at a solution to #3 so far are convoluted and seem to run
in O(lg**2 n) time.  I must be missing something, either obvious or obscure.
Any hints that would steer me in the right direction without totally
giving away the solution would be greatly appreciated.

Thanks,
Michael (claym@wolf.cs.washington.edu)
(But post your answers to cs421 distribution.)

------- End of Forwarded Message
Date: 7 Feb 1996 11:55 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
wednesday office hour CANCELED

I have to cancel my office hour today.  I'll try to be available
tomorrow or friday to compensate; details later.
To: cse421@geoduck
Subject:
HW5, problem 2(a)
Date: Thu, 08 Feb 1996 11:05:56 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

A student pointed out that we didn't discuss ties in the greedy method, that
is, what the method does when pqr = qrs.  To disambiguate the problem, you may
assume in part (a) that pqr < qrs, and just give the necessary and sufficient
conditions assuming this inequality is true.  This ensures that there are no
ties, and also eliminates the need to discuss the completely dual case when
pqr > qrs.  In part (b), make sure you choose values that satisfy pqr < qrs.
Date: Thu, 8 Feb 1996 14:51:23 -0800
From:
aberman (Andrew P. Berman)
To: cse421
Subject:
Average Homework Grades

I have not included the grades of some people whom I suspect have dropped
the course.  I also only included the grades for homeworks submitted.  So
these averages (like all averages) must be taken with a grain of salt.

Hw1:  19 out of 25
Hw2:  22 out of 25
Hw3:  22 out of 30

Andy
Date: 9 Feb 1996 08:55 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
office hours

To make up for my canceled office hour wednesday, I'll be available
intermittently today for questions.  I'm tied up from 12:30 - 2:00,
and need to be in and out a bit during the rest of the day, but will
be happy to answer questions if I'm in.
To: cse421@geoduck
Subject:
Cocke, Kasami, Younger algorithm
Date: Fri, 09 Feb 1996 12:04:07 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

I mentioned in class that there is another dynamic programming algorithm that
is extremely similar in form to the one we are looking at now for matrix chain
product and polygon triangulation.  It is a cubic algorithm for testing
context-free language membership.  If you are interested in seeing it, there
is a PostScript writeup in the following URL:

http://www.cs.washington.edu/education/courses/322/94a/cky.ps

I have a feeling that there should be some unification of that problem and
polygon triangulation, just because of the similarity of these algorithms, but
I have not discovered it.  If you have ideas along these lines, I'd love to
hear them.
Date: Fri, 9 Feb 1996 16:17:50 -0800 (PST)
From:
Robert Oganyan <ogan@grizzly.cs.washington.edu>
To: cse421@grizzly.cs.washington.edu
Subject:
mailing list

Please remove me from the mailing list

Robert
Date: Sun, 11 Feb 1996 16:41:38 -0800 (PST)
From:
"D. Godon" <dgodon@u.washington.edu>
To: Martin Tompa <tompa@cs.washington.edu>
Cc: cse421@geoduck.cs.washington.edu
Subject:
Re: Cocke, Kasami, Younger algorithm

On problem 3a, can we assume k is an integer?  If not, it would seem part
a and b are the same.

Thanks
To: cse421@geoduck
Subject:
HW5, problem 3a
Date: Mon, 12 Feb 1996 08:52:02 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Yes, you should assume k is an integer.

------- Forwarded Message

Date:    Sun, 11 Feb 1996 16:41:38 -0800
From:    "D. Godon" <dgodon@u.washington.edu>
To:      Martin Tompa <tompa@cs.washington.edu>
cc:      cse421@geoduck.cs.washington.edu
Subject: Re: Cocke, Kasami, Younger algorithm


On problem 3a, can we assume k is an integer?  If not, it would seem part
a and b are the same.

Thanks

------- End of Forwarded Message
To: cse421@geoduck
Subject:
next reading assignment
Date: Mon, 12 Feb 1996 10:56:07 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Read the introduction to Chapter 26, and all of Section 26.2.
Date: Mon, 12 Feb 1996 12:40:26 -0800
From:
aberman@hobbes (Andrew Berman)
To: cse421@hobbes
Subject:
Average for HW#4

40

Andy
To: cse421@paintbrush.cs.washington.edu
Subject:
Mistake in Homework Grading
Date: Wed, 14 Feb 1996 09:41:25 PST
From:
Andrew Berman <aberman@paintbrush.cs.washington.edu>

HW#4 (the one we just gave back)

Problem 10-1 Largest i numbers in sorted order:

I took off credit for those who gave the answer n lg n.
Both n lg n and n lg n + i are legitimate answers---n lg n is actually
the better answer.

What *is* still wrong is claiming 'i' is a constant--you won't get any
points back if you did that.

However, if you wrote n lg n but did not claim 'i' was a constant, please
bring your homework in either to Friday's class or to my office hours
to get credit for your answer.

Andy

_______________________________________________________________________________
|    Andrew P. Berman     |Dept. Of Computer Science, University Of Washington|
|aberman@cs.washington.edu|  http://www.cs.washington.edu/homes/aberman       |
|_________________________|___________________________________________________|
To: cse421@geoduck
Subject:
HW6 on the web
Date: Mon, 19 Feb 1996 22:58:06 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

HW6 will be handed out in Wednesday's lecture, due a week later.  It
is on the web, if you want to get a headstart.
Date: Thu, 22 Feb 1996 08:20:09 -0800
From:
aberman@hobbes (Andrew Berman)
To: cse421@hobbes
Subject:
Office Hours *CANCELLED* This Monday and Tuesday

Sorry, but I'm gonna be out for a week starting today and lasting until
next Thursday.

Andy
To: cse421@geoduck
Subject:
HW6, #2 (Warning: this message contains a hint.)
Date: Thu, 22 Feb 1996 14:07:45 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

One student, stuck on problem 2, asked for an idea of how to start.  Think
about finding the edit distance between the prefixes x[1..i] and y[1..j].
Date: Sun, 25 Feb 1996 09:57:56 -0800 (PST)
From:
Eva Marie Allen <eallen@wolf.cs.washington.edu>
To: Algorithms <cse421@cs.washington.edu>
Subject:
last problem

I have a problem.  I need to prove that Warshall' algorithm works, and I
don't believe it does.  It we need to take the min of d(i,j) and d(i,k) +
d(k,j), well, if d(i,j) is zero, and the sum of the rest is positive,
then the min is zero.  Right?  So I can easily see creating an all zero
matrix.  The implication of the algorithm is that if d(i,j) is 1, then
either d(i,k) or d(k,j) is one.  I do not find this to be true.  The book
SAYS it is true, so I assume that I am looking at this the wrong way.
How DOES this work?

Thanks,

Eva

*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
Eva Marie Allen                      |  Though it cost all you have, get
eallen@cs.washington.edu             |  understanding.  Esteem her, and she
UW Computer Science Department       |  will exalt you; embrace her, and she
In the beautiful city of Seattle     |  will honor you.              Solomon
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
Only 182 days until my wedding! (8/25/96)
Date: 25 Feb 1996 13:12 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: Eva Marie Allen <eallen@wolf.cs.washington.edu>
Subject:
Re: last problem
Cc: Algorithms <cse421@cs.washington.edu>
> ... we need to take the min of d(i,j) and d(i,k) +
> d(k,j), well, if d(i,j) is zero, and the sum of the rest is positive,
> then the min is zero.  Right?  So I can easily see creating an all zero
> matrix.  The implication of the algorithm is that if d(i,j) is 1, then
> either d(i,k) or d(k,j) is one.  I do not find this to be true.  The book
> SAYS it is true, so I assume that I am looking at this the wrong way.
^^^^
where?

edge weights/distances can be positive, negative or zero.  zero
means a free trip from point i to point j; if that's cheaper than
going via Kansas City, then that's what you should do.  E.g., bus
fares between stops  in downtown seattle would give a matrix full of
zero's.  [Note that in the transitive closure algorithm on the same
page, zero means NO path from i to j, but it's not computed the same
way.]
To: cse421@geoduck
Subject:
last problem
Date: Mon, 26 Feb 1996 09:29:39 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

To amplify on Larry's answer a bit, Floyd's algorithm (for shortest paths) and
Warshall's algorithm (for transitive closure) are very similar in spirit, but
not at all identical.

"It we need to take the min of d(i,j) and d(i,k) +
d(k,j), well, if d(i,j) is zero, and the sum of the rest is positive,
then the min is zero.  Right?  So I can easily see creating an all zero
matrix."

This is a statement about the shortest paths algorithm.  What it says is that,
once d(i,j) is zero, it will never go positive again.  (Actually, this is true
more generally: no matter what value d(i,j) has, it will never later
increase.)  But it doesn't follow from this that the algorithm will
necessarily create an all zero matrix.  In general, it won't.

"The implication of the algorithm is that if d(i,j) is 1, then
either d(i,k) or d(k,j) is one.

This is a statement about the transitive closure algorithm, although it should
say "if d(i,j) changes from 0 to 1, then either d(i,k) or d(k,j) is one."  In
class we never talked about the transitive closure (Warshall's) variant.
Date: Mon, 26 Feb 1996 10:16:23 -0800 (PST)
From:
Michael Clay <claym@wolf.cs.washington.edu>
To: Martin Tompa <tompa@cs.washington.edu>
cc: cse421@geoduck.cs.washington.edu
Subject:
Re: HW6, #2 (Warning: this message contains a hint.)

Where the text says "describe an algorithm",
how detailed a description are we supposed to
give?  Please specify _exactly_ what you do
and do not expect in the description.

Thanks,
Michael (claym@wolf.cs.washington.edu)
To: cse421@geoduck
Subject:
"describe an algorithm"
Date: Mon, 26 Feb 1996 10:24:10 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Ideally this should be done in the style of the textbook: a precise algorithm
described in pseudocode (see e.g. Matrix-Chain-Order or LCS-Length), plus a
short English description of how the algorithm works in general and any tricky
portions of it in particular.  If correctness of your algorithm isn't obvious,
part of this description should convince the reader of its correctness.

------- Forwarded Message

Date:    Mon, 26 Feb 1996 10:16:23 -0800
From:    Michael Clay <claym@wolf.cs.washington.edu>
To:      Martin Tompa <tompa@cs.washington.edu>
cc:      cse421@geoduck.cs.washington.edu
Subject: Re: HW6, #2 (Warning: this message contains a hint.)

Where the text says "describe an algorithm",
how detailed a description are we supposed to
give?  Please specify _exactly_ what you do
and do not expect in the description.

Thanks,
Michael (claym@wolf.cs.washington.edu)

------- End of Forwarded Message
To: cse421
Subject:
space
Date: Mon, 26 Feb 1996 23:17:43 PST
From:
Martin Tompa <tompa@june.cs.washington.edu>

A student asked how to measure the space required by an algorithm.

Generally, the space is the maximum number of words of memory needed
by the algorithm at any one time.  Most dynamic programming algorithms
need much more space than the space used by the inputs and outputs.
For instance, matrix-chain-order needs space Theta(n^2) for the m and
s tables, even though the input and output only take up Theta(n)
space.
To: cse421
Subject:
HW6, #4
Date: Mon, 26 Feb 1996 23:56:02 PST
From:
Martin Tompa <tompa@june.cs.washington.edu>

On problem 4 of the homework sheet, I gave the following useful bit of
advice:

"Note that what is computed in line 6 during iteration k is not
exactly the value d_{ij}^(k)."

One of the students has convinced me that this advice is ... uh
... err ... not, strictly speaking, exactly, completely true.

But no matter: if you believe the advice, you will be led to the proof
that I had in mind.  If you don't believe the advice, you will be led
to the proof this student came up with.

This is what is so great about teaching: no matter how often you teach
one of these classical results, each time you learn something new
about it, often from students.
To: cse421@geoduck
Subject:
details on Chapter 36 reading assignment
Date: Sun, 03 Mar 1996 14:44:14 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

You should read the whole chapter, but you can skim the following
parts:

from Lemma 36.1 to the end of Section 36.1
the proofs of Theorems 36.9 and 36.10

The main technical meat that we want to cover is in Section 36.5, but
you need some of the motivation and definitions from earlier in the
chapter to make sense of this.  On a first reading, I would strongly
recommend delaying the reading from Lemma 36.5 to the end of Section
36.3 until you've finished the chapter.  These are important results
and proofs, but you will have trouble understanding them until you get
more experience with reducibilities in Section 36.5.
Date: 3 Mar 1996 21:03 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
24-1(a)

> Please define a second best minimum spanning tree: suppose there are
> two MSTs-- does one qualify as a 2'nd best? Or do you want to see the
> one with the second-smallest DISTINCT weight?

I'm willing to credit a good solution to the problem under either
interpretation.  Howeverm, I think the book intended, and I think it
will be easier to prove, the first interpretation: if there happen
to be several spanning trees with weight equal to the minimum, then
T can be any of them, and any of the others can be taken as the "2nd
best".
Date: 3 Mar 1996 21:33 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
Subject:
Re: 24-1(a)
To: cse421@cs
PS: In line with the previous message, you should change "the
second-best" to "a second-best" in part (c), in the case where the
second-best is not unique.
Date: 3 Mar 1996 21:36 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
HW7, 17.2-4

> Could I make the assumption that the longest distance possible between
> two gas stations is n miles, the same miles as the professor's car can run
> with a full tank of gas?

Yes, you may assume there is no section of road longer than n miles
without a gas station.
Date: 4 Mar 1996 11:44 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
HOMEWORK REVISION

Problem 24-1(a) is trickier than I had first realized.  It's a good
test of your understanding of min spanning trees, and not beyond
what I think you can do, but the assignment seems meaty enough
without it.  So I'm making part (a) EXTRA CREDIT.  Parts (b) and (c)
are STILL REQUIRED, and of course you can use the result stated in
part (a)  even if you don't prove it.

****** WARNING:  HINT FOLLOWS *******



In case you want a start on the extra credit part, here's a hint  --








Let T1 be a min spanning tree.
Let T2 be a 2nd best spanning tree with as many edges in common with
T1 as possible.
Date: 4 Mar 1996 18:20 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
OFFICE HOUR CHANGE

Unfortunately, I will be unavailable during my normal office hour,
2:30-3:30 tomorrow, Tuesday 3/5.  However, I should be in most of
the morning, and from 1:30 to 2:30.  Sorry for the inconvenience.
Date: 5 Mar 1996 10:59 PST
From:
Larry Ruzzo <ruzzo@quinault.cs.washington.edu>
To: cse421@cs
Subject:
Re: HW 7, 17-2.2  (***CAUTION; HINT BELOW***)

> 	I am have some trouble coming up with an algorithm that runs in
> O(nW) for this problem.  I can see using variations of Floyd-Warshall or
> Matrix Chain Order, but these seem to take O(n**3).  I haven't been able
> to come up with a variation of Longest Common Subsequence that works.  Is
> there something I am missing?

(***CAUTION; HINT BELOW***)

The key issue with dynamic programming is that an optimal solution
to the problem can be built out of optimal solutions to related
smaller problems.  The smaller problems often arise by fixing in
various ways some of the choices that ultimately must be made.
E.g., in matrix parenthesization, there was a choice as to where the
outermost parentheses go, and assuming they go at position k,
certain subproblems arise.  You don't know k, but can consider all
possibilities.  In the string edit problem you did for homework,
there's a choice as to the last edit operation used to build the
output string from the input string, and assuming it was, say, a
copy, then a certain subproblem  arises; assuming it was a twiddle,
a different subproblem arises, etc.  Again, you don't know which is
the right choice to make for the last operation, so you consider
them all.  LCS is similar.

What are the choices in Knapsack?  If you fix one, does it give rise
to a smaller knapsack-like problem?  Which choice should you fix?
How many ways are there to fix it?  Can you tell what the right
choice is?  If not, can you consider all possibilities in some
systematic way?

If you're stuck, I always recommend playing with some examples.
Make up some simple instances of the problem with 1 object; what's
it's solution?  Vary W; how does the solution change?  Add another
object or 3, etc.  There's of course some danger that your examples
will be too simple or too specialized or that you will leap to the
wrong conclusion from them, but that's what proofs are for, and
concrete examples often help crystalize your thinking.
To: cse421@geoduck
Subject:
evaluations Friday
Date: Wed, 06 Mar 1996 13:50:08 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Don't forget your #2 pencils on Friday.  We'll take the last 10 minutes of
class to do student evaluations.
To: cse421@geoduck
Subject:
HW8 (just kidding)
Date: Thu, 07 Mar 1996 17:19:16 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Here are some exercises on Chapter 36 to help you study for the final.  You
don't need to turn these in.

p. 923, 36.1-1
p. 924, 36.1-4
p. 929, 36.2-5
p. 938, 36.3-1
p. 960, 36.5-1
Date: Fri, 8 Mar 1996 15:03:37 -0800
From:
aberman@hobbes (Andrew Berman)
To: cse421@hobbes
Subject:
Office Hours During Finals Week

I will be holding office hours in 326 from 11:30-1:30 on Monday.

Andy
Date: Wed, 13 Mar 1996 12:55:48 -0800 (PST)
From:
Larry Roske <roske@lynx.cs.washington.edu>
To: Martin Tompa <tompa@cs.washington.edu>
cc: cse421@geoduck.cs.washington.edu
Subject:
Re: HW8 (just kidding)

Will problems like those suggested for Ch. 36 be on the
final, and if so would it be possible to get solutions
to the suggested problems via Email?

Thanks,

--Larry


On Thu, 7 Mar 1996, Martin Tompa wrote:

>
> Here are some exercises on Chapter 36 to help you study for the final.  You
> don't need to turn these in.
>
> p. 923, 36.1-1
> p. 924, 36.1-4
> p. 929, 36.2-5
> p. 938, 36.3-1
> p. 960, 36.5-1
>
Date: Wed, 13 Mar 1996 13:08:36 -0800 (PST)
From:
Jack Hill <jshill@u.washington.edu>
To: Larry Roske <roske@cs.washington.edu>
Cc: Martin Tompa <tompa@cs.washington.edu>, cse421@geoduck.cs.washington.edu
Subject:
Re: HW8 (just kidding)

I would also appreciate solutions for the chapter 36 problems,
especially if they are along the lines of those that we'll see on the final.

Just wanted to let you know that more than one person was interested!

Thanks,
Jack Scott Hill
jshill@cs


On Wed, 13 Mar 1996, Larry Roske wrote:

> Will problems like those suggested for Ch. 36 be on the
> final, and if so would it be possible to get solutions
> to the suggested problems via Email?
>
> Thanks,
>
> --Larry
>
>
> On Thu, 7 Mar 1996, Martin Tompa wrote:
>
> >
> > Here are some exercises on Chapter 36 to help you study for the final.  You
> > don't need to turn these in.
> >
> > p. 923, 36.1-1
> > p. 924, 36.1-4
> > p. 929, 36.2-5
> > p. 938, 36.3-1
> > p. 960, 36.5-1
> >
>
To: cse421@geoduck
Subject:
HW8 sketchy solutions
Date: Wed, 13 Mar 1996 14:17:40 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Yes, the intent of HW8 was to give you an idea of the type of problem you
should be able to answer on the final tomorrow.

Because this request comes so late, there isn't time to write up and broadcast
detailed solutions.  What I can do at this point is try to outline the key
ideas in the solutions, leaving the details for you to work out.

36.1-1: One direction of this is easy.  For the harder direction, use a binary
search on the parameter k.

36.1-4: The instructions you received for HW7 gave the key issue for this
problem.  If W is input in binary (as usual), then nW is exponential in the
length of this binary encoding.

36.2-5: Look at the definition of NP on page 927.  To determine if x is in L,
try all possible values of y up to length |x|^c.  There are about 2^{|x|^c+1}
such strings.

36.3-1: Let f1 be the reduction function for the reduction from L1 to L2, and
f2 be the reduction function from L2 to L3.  Then the composition f2 o f1
reduces L1 to L3.  You have to argue that this composition runs in polynomial
time, which is done as in the proof of Lemma 36.3.

36.5-1: There are two parts to this proof.

a. (The problem is in NP.)  In addition to G1=(V1,E1) and G2=(V2,E2), the
verification algorithm takes a function f: V1 -> V2.  The algorithm checks
that f is total (i.e., defined on all vertices of V1), an injection (i.e.,
one-one), and that {u,v} in V1 implies {f(u),f(v)} in V2.  You must show that
all this can be done in polynomial time.

b. (The problem is NP-hard.  Here's a reduction from CLIQUE.)  Given a graph G
and an integer k for which you want to know if G has a k-clique, construct
G2=G and G1 = a k-clique.  You must show that these can be constructed from G
and k in polynomial time, and that G has a k-clique iff G1 is a subgraph of
G2.


------- Forwarded Message

Date:    Wed, 13 Mar 1996 12:55:48 -0800
From:    Larry Roske <roske@lynx.cs.washington.edu>
To:      Martin Tompa <tompa@cs.washington.edu>
cc:      cse421@geoduck.cs.washington.edu
Subject: Re: HW8 (just kidding)

Will problems like those suggested for Ch. 36 be on the
final, and if so would it be possible to get solutions
to the suggested problems via Email?

Thanks,

- --Larry


On Thu, 7 Mar 1996, Martin Tompa wrote:

>
> Here are some exercises on Chapter 36 to help you study for the final.  You
> don't need to turn these in.
>
> p. 923, 36.1-1
> p. 924, 36.1-4
> p. 929, 36.2-5
> p. 938, 36.3-1
> p. 960, 36.5-1
>

------- End of Forwarded Message
To: cse421@geoduck
Subject:
HW7
Date: Wed, 13 Mar 1996 15:16:08 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Andy doesn't quite have your HW7's graded, so you'll have to look at the
solutions he prepared if you want to see how to solve them.  Hopefully you can
pick up your graded HW7 at the exam tomorrow.  Sorry we couldn't get them back
to you sooner.
To: cse421@geoduck
Subject:
Problem 36.1-4
Date: Wed, 13 Mar 1996 15:26:46 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

No, that's not it.  It's really even simpler.  The input to the problem
consists of the n weights and values, plus W written in binary.  Imagine that
n is small, and the weights and W are large; let's forget about the values of
the objects for this discussion.  Now the size of the input is O(n log W),
since there are n+1 weights (including W itself), each of length at most log
W.  But the running time is nW, which is not polynomial in n log W.  In fact,
if n is a constant and W is allowed to vary, then nW is exponential in n log
W, because W is exponential in log W.

------- Forwarded Message

Date:    Wed, 13 Mar 1996 15:01:21 -0800
To:      tompa@cs
Subject: Problem 36.1-4


I have a question regarding 36.1-4:

If W is input in binary, then nW is exponential in the
length of this binary encoding.

Is this because there are 2^n possible combinations of
n items to consider, and therefore the capacity of W,
if unrestricted, could be of a size that is exponential
in n?


------- End of Forwarded Message
Date: Wed, 13 Mar 1996 19:42:28 -0800 (PST)
From:
Eva Marie Allen <eallen@wolf.cs.washington.edu>
To: Martin Tompa <tompa@geoduck.cs.washington.edu>,
X says Y means Z <ruzzo@cs.washington.edu>,
Algorithms <cse421@cs.washington.edu>
Subject:
NP

I was wondering how much weight NP Completeness carried on this final.  I
am reading the material, and could easily spend all night on it.

Eva
Date: Thu, 14 Mar 1996 13:47:23 -0800
From:
aberman@hobbes (Andrew Berman)
To: cse421@hobbes
Subject:
Hw7

I'll place them in a box in room 326 tomorrow morning.
Andy
To: cse421@geoduck
Subject:
HW7 and final exam pickup
Date: Fri, 15 Mar 1996 14:49:25 PST
From:
Martin Tompa <tompa@geoduck.cs.washington.edu>

Rather than putting HW7 in a box for pickup, we've decided to make both it and
the final exams available together.  On Monday, you can pick these up from
Larry's office, Sieg 415.




{ ruzzo | tompa | aberman } @cs.washington.edu
(Last Update:
01/05/96)


